# Generative Medical Imaging Laboratory (GMI)

The **Generative Medical Imaging Laboratory (GMI)** is a comprehensive Python package designed for advanced computational imaging research, with a focus on medical imaging applications. GMI provides a complete toolkit for linear algebra operations, stochastic differential equations, diffusion models, and image reconstruction tasks, all built on PyTorch.

## ğŸš€ Quick Start

### Using Docker (Recommended)

The easiest way to get started with GMI is using Docker, which provides a pre-configured environment with all dependencies.

```bash
# Clone the repository
git clone https://github.com/Generative-Medical-Imaging-Lab/gmi.git
cd gmi

# Build and start the Docker container
docker compose up -d

# Execute commands in the container
docker exec -it gmi-container bash
```

### Direct Installation

```bash
# Install from GitHub
pip install git+https://github.com/Generative-Medical-Imaging-Lab/gmi.git

# Or install in development mode
git clone https://github.com/Generative-Medical-Imaging-Lab/gmi.git
cd gmi
pip install -e .
```

## ğŸ“ Directory Structure

```
gmi_base/                          # Root directory (git clone location)
â”œâ”€â”€ Dockerfile                      # Docker configuration
â”œâ”€â”€ docker-compose.yml             # Docker Compose configuration
â”œâ”€â”€ setup.py                       # Package installation script
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ main.py                        # Command-line interface entry point
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ LICENSE                        # MIT License
â”œâ”€â”€ gmi/                           # Main package source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ commands/                  # CLI commands
â”‚   â”‚   â””â”€â”€ visualize_dataset.py
â”‚   â”œâ”€â”€ datasets/                  # Dataset implementations
â”‚   â”‚   â”œâ”€â”€ core.py               # Base dataset classes
â”‚   â”‚   â”œâ”€â”€ mnist.py              # MNIST dataset
â”‚   â”‚   â”œâ”€â”€ medmnist.py           # MedMNIST datasets
â”‚   â”‚   â””â”€â”€ synthrad2023.py       # SynthRAD2023 dataset
â”‚   â”œâ”€â”€ diffusion/                 # Diffusion model implementations
â”‚   â”‚   â”œâ”€â”€ core.py
â”‚   â”‚   â”œâ”€â”€ unconditional.py
â”‚   â”‚   â””â”€â”€ diffusion_posterior.py
â”‚   â”œâ”€â”€ distribution/              # Probability distributions
â”‚   â”‚   â”œâ”€â”€ core.py
â”‚   â”‚   â”œâ”€â”€ gaussian.py
â”‚   â”‚   â”œâ”€â”€ lognormal.py
â”‚   â”‚   â””â”€â”€ uniform.py
â”‚   â”œâ”€â”€ linalg/                    # Linear algebra utilities
â”‚   â”‚   â”œâ”€â”€ core.py
â”‚   â”‚   â”œâ”€â”€ fourier.py
â”‚   â”‚   â”œâ”€â”€ interp.py
â”‚   â”‚   â”œâ”€â”€ misc.py
â”‚   â”‚   â”œâ”€â”€ permute.py
â”‚   â”‚   â”œâ”€â”€ polar.py
â”‚   â”‚   â””â”€â”€ sparse.py
â”‚   â”œâ”€â”€ loss_function/             # Loss functions
â”‚   â”‚   â””â”€â”€ inv_t_weighted_mse.py
â”‚   â”œâ”€â”€ lr_scheduler/              # Learning rate schedulers
â”‚   â”‚   â””â”€â”€ linear_warmup.py
â”‚   â”œâ”€â”€ network/                   # Neural network architectures
â”‚   â”‚   â”œâ”€â”€ densenet.py
â”‚   â”‚   â”œâ”€â”€ diffusers_unet_2D.py
â”‚   â”‚   â”œâ”€â”€ simplecnn.py
â”‚   â”‚   â”œâ”€â”€ lambda_layer.py
â”‚   â”‚   â””â”€â”€ diffusion/
â”‚   â”‚       â””â”€â”€ MedMNISTDiffusion.py
â”‚   â”œâ”€â”€ samplers/                  # Sampling utilities
â”‚   â”‚   â”œâ”€â”€ core.py
â”‚   â”‚   â”œâ”€â”€ dataloader_sampler.py
â”‚   â”‚   â”œâ”€â”€ dataset_sampler.py
â”‚   â”‚   â””â”€â”€ module_sampler.py
â”‚   â”œâ”€â”€ sde/                       # Stochastic differential equations
â”‚   â”‚   â”œâ”€â”€ core.py
â”‚   â”‚   â”œâ”€â”€ diagonal.py
â”‚   â”‚   â”œâ”€â”€ fourier.py
â”‚   â”‚   â””â”€â”€ scalar.py
â”‚   â”œâ”€â”€ tasks/                     # Task implementations
â”‚   â”‚   â””â”€â”€ reconstruction.py      # Image reconstruction tasks
â”‚   â””â”€â”€ train/                     # Training utilities
â”‚       â””â”€â”€ __init__.py            # Training functions
â”œâ”€â”€ examples/                      # Example scripts and configurations
â”‚   â”œâ”€â”€ visualize_all_datasets/    # Dataset visualization examples
â”‚   â”œâ”€â”€ medmnist_generation/       # MedMNIST generation examples
â”‚   â”œâ”€â”€ medmnist_restoration/      # MedMNIST restoration examples
â”‚   â”œâ”€â”€ medmnist_restoration_from_config/
â”‚   â”œâ”€â”€ mnist_denoising/           # MNIST denoising examples
â”‚   â””â”€â”€ test_reconstruction_yaml.py
â””â”€â”€ gmi_data/                      # Data storage directory
    â”œâ”€â”€ datasets/                  # Downloaded datasets
    â”‚   â”œâ”€â”€ MNIST/                 # MNIST dataset files
    â”‚   â””â”€â”€ MedMNIST/              # MedMNIST dataset files
    â”œâ”€â”€ models/                    # Trained model checkpoints
    â””â”€â”€ outputs/                   # Output files and visualizations
        â””â”€â”€ visualizations/        # Generated visualizations
```

## ğŸ³ Docker Usage

### Starting the Container

```bash
# Build and start the container
docker compose up -d

# Check container status
docker compose ps
```

### Executing Commands

```bash
# Run a single command
docker exec gmi-container python -c "import gmi; print('GMI loaded successfully!')"

# Start an interactive shell
docker exec -it gmi-container bash

# Run the visualization script (downloads all datasets)
docker exec gmi-container bash examples/visualize_all_datasets/visualize_all_datasets.sh

# Run a specific example
docker exec gmi-container python examples/test_reconstruction_yaml.py
```

### Data Persistence

The `gmi_data/` directory is mounted as a volume, so:
- Downloaded datasets persist between container restarts
- Model checkpoints are saved to `gmi_data/models/`
- Visualizations are saved to `gmi_data/outputs/visualizations/`

### GPU Support

The Docker configuration includes NVIDIA GPU support. Ensure you have:
- NVIDIA Docker runtime installed
- Compatible NVIDIA drivers
- Docker configured for GPU access

## ğŸ¯ Core Features

### 1. Image Reconstruction Tasks

GMI provides a comprehensive framework for image reconstruction tasks:

```python
from gmi.tasks.reconstruction import ImageReconstructionTask

# Create a reconstruction task from YAML configuration
task = ImageReconstructionTask.from_config('config.yaml', device='cuda')

# Sample images, measurements, and reconstructions
images, measurements, reconstructions = task(
    image_batch_size=4,
    measurement_batch_size=1,
    reconstruction_batch_size=1
)
```

### 2. Training Framework

Built-in training utilities with support for:
- Early stopping
- Exponential Moving Average (EMA)
- WandB logging
- Model checkpointing
- Learning rate scheduling

```python
from gmi.train import train
from gmi.tasks.reconstruction import ImageReconstructionTask

# Load task and create loss closure
task = ImageReconstructionTask.from_config('config.yaml')
loss_closure = task.loss_closure(loss_fn)

# Train the model
train_losses, val_losses, eval_metrics = train(
    train_loader=train_loader,
    loss_closure=loss_closure,
    num_epochs=100,
    validation_loader=val_loader,
    use_ema=True,
    wandb_project="gmi-reconstruction",
    save_best_model_path="best_model.pth"
)
```

### 3. Dataset Support

Comprehensive dataset support including:
- **MNIST**: Standard handwritten digits dataset
- **MedMNIST**: 12 medical imaging datasets with multiple sizes (28x28, 64x64, 128x128, 224x224)
  - PathMNIST, ChestMNIST, DermaMNIST, OCTMNIST
  - PneumoniaMNIST, RetinaMNIST, BreastMNIST, BloodMNIST
  - TissueMNIST, OrganAMNIST, OrganCMNIST, OrganSMNIST
- **SynthRAD2023**: Synthetic CT dataset

### 4. Neural Network Architectures

Pre-implemented architectures:
- SimpleCNN: Lightweight convolutional network
- DenseNet: Dense connection architecture
- Diffusers UNet 2D: Diffusion model backbone
- Custom lambda layers for specialized operations

### 5. Diffusion Models

Complete diffusion model implementation:
- Unconditional diffusion models
- Conditional diffusion models
- Various sampling strategies
- Training utilities

## ğŸ“Š Getting Started Examples

### 1. Visualize All Datasets

This is a great place to start as it downloads all available datasets:

```bash
# Run the comprehensive visualization script
docker exec gmi-container bash examples/visualize_all_datasets/visualize_all_datasets.sh
```

This script will:
- Download all MedMNIST variants in multiple sizes
- Generate visualizations for each dataset
- Save results to `gmi_data/outputs/visualizations/`

### 2. Test Reconstruction Task

```bash
# Test the reconstruction task with YAML configuration
docker exec gmi-container python examples/test_reconstruction_yaml.py
```

### 3. MNIST Denoising

```bash
# Run MNIST denoising example
docker exec gmi-container python examples/mnist_denoising/main.py
```

### 4. MedMNIST Restoration

```bash
# Run MedMNIST restoration with configuration
docker exec gmi-container python examples/medmnist_restoration_from_config/main.py
```

## ğŸ”§ Configuration

GMI uses YAML configuration files for defining reconstruction tasks:

```yaml
# Example config.yaml
dataset:
  class: gmi.datasets.MNIST
  params:
    train: true
    download: true
    images_only: false

measurement_simulator:
  class: gmi.distribution.gaussian.AdditiveWhiteGaussianNoise
  params:
    noise_standard_deviation: 0.1

image_reconstructor:
  class: gmi.network.SimpleCNN
  params:
    input_channels: 1
    output_channels: 1
    hidden_channels_list: [16, 32]
    activation: relu
    dim: 2
```

## ğŸ§ª Training and Evaluation

### Training a Model

```python
import torch
from gmi.tasks.reconstruction import ImageReconstructionTask
from gmi.train import train

# Load task from configuration
task = ImageReconstructionTask.from_config('config.yaml', device='cuda')

# Create loss function
loss_fn = torch.nn.MSELoss()

# Create loss closure
loss_closure = task.loss_closure(loss_fn)

# Train the model
train_losses, val_losses, eval_metrics = train(
    train_loader=train_loader,
    loss_closure=loss_closure,
    num_epochs=100,
    num_iterations=100,
    validation_loader=val_loader,
    use_ema=True,
    early_stopping=True,
    wandb_project="gmi-experiment",
    save_best_model_path="best_model.pth"
)
```

### Evaluating a Model

```python
# Load trained model
model = task.image_reconstructor
model.load_state_dict(torch.load('best_model.pth'))

# Evaluate on test set
model.eval()
with torch.no_grad():
    test_images, test_measurements, test_reconstructions = task(
        image_batch_size=32,
        measurement_batch_size=1,
        reconstruction_batch_size=1
    )
    
    # Calculate metrics
    mse = torch.mean((test_reconstructions - test_images) ** 2)
    print(f"Test MSE: {mse.item():.4f}")
```

## ğŸ“ˆ Monitoring and Logging

### WandB Integration

```python
# Enable WandB logging during training
train_losses, val_losses, eval_metrics = train(
    train_loader=train_loader,
    loss_closure=loss_closure,
    wandb_project="gmi-reconstruction",
    wandb_config={
        "model": "SimpleCNN",
        "dataset": "MNIST",
        "noise_level": 0.1
    }
)
```

### Custom Evaluation Functions

```python
def custom_eval_fn(model, wandb_project, wandb_config, epoch):
    """Custom evaluation function called during training."""
    model.eval()
    with torch.no_grad():
        # Run evaluation
        test_loss = evaluate_model(model, test_loader)
        
        # Log to WandB
        if wandb_project:
            wandb.log({
                "test_loss": test_loss,
                "epoch": epoch
            })
        
        return {"test_loss": test_loss}

# Use in training
train_losses, val_losses, eval_metrics = train(
    train_loader=train_loader,
    loss_closure=loss_closure,
    eval_fn=custom_eval_fn,
    epochs_per_evaluation=10
)
```

## ğŸ” Command Line Interface

GMI provides a CLI for common tasks:

```bash
# Visualize a specific dataset
gmi visualize-dataset --dataset mnist
gmi visualize-dataset --dataset PathMNIST_128_train

# List available commands
gmi --help
```

## ğŸ“¦ Requirements

### System Requirements
- Python 3.10 or higher
- CUDA-compatible GPU (recommended)
- Docker (for containerized usage)

### Key Dependencies
- **PyTorch**: Deep learning framework
- **TorchVision**: Computer vision utilities
- **Diffusers**: Diffusion model implementations
- **MedMNIST**: Medical imaging datasets
- **WandB**: Experiment tracking
- **PyYAML**: Configuration file parsing
- **Click**: Command-line interface

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

For questions and support:
- Open an issue on GitHub
- Check the examples directory for usage patterns
- Review the test files for implementation details

---

**For AI Agents**: This package provides a complete framework for medical image reconstruction and generation tasks. The `gmi_data/` directory structure is automatically created and managed by the package. Use Docker for consistent environments, and start with the visualization script to download all datasets. The training framework supports various architectures and can be extended with custom components through YAML configuration files.

